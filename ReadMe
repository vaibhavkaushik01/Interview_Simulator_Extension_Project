Chrome Extension for Interview Simulation using llm model

Functionalities :-
1. An icon will show on leetcode when we click on a question to solve.
2. Extension should have access to problem statement(context) and users code.
3. Once clicked Extension will ask to read (it will give some time to read) and ask relevant questions.
4. After that it will ask user his approach to solve.
5. Once user tell a approach it will ask user to code the approach and check whether its correct or not.
6. If not correct it can give clues to user.
7. If the ans is correct it will ask for optimizations if present in that question.
8. Finally feedback to user is provided by this extension.

Functionalities :-
1. Demo mode(with my api key)
2. Normal mode(user will have to provide their api key generated from google ai studio)
2. Speech to text and text to speech(handled using webspeech api)
3. Call to gemini api for llm response

Components :-
1. popup html :- 
  > Ask for Open AI key from user(for normal mode).
  >Will give access for one usecase(in demo mode).
  > If key is provided store it on local browser of user.
  > Show a turn on and off button.

2. Permissions :-
  > Declared in manifest.json to specify what your extension can access, such as:
  >"activeTab" – interact with the current tab
  >"storage" – save user preferences
  >"scripting" – for injecting scripts

3. Content Script :-
  >Inject UI overlays or floating widgets
  >Read code from code editors
  >Modify elements

.....


Working of extension: 
> when injected icon on bottom of page is clicked problem description is sent from content script to background script which then call server sending a prompt and getting llm response from server.

> This response is then sent back to content script which have access to window object and webspeech api is used to utter this in content script.(response is a greeting and some problem desciption along with asking user's approach to question).

> User can then click the record button and record his response and click record button again to end recording. This response is then converted to text using webspeech api and sent to background script.

> background script again form a prompt along with user code, user interaction and conversation history and call server for response and the response is sent back to content script where it is uttered.

Working of server:
> Server just has access to api key and call gemini through sdk and return reponse to background_script.


